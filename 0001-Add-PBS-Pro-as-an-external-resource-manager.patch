From 2a306c59ccb1f3360b92dad6a21c9642396ed5d7 Mon Sep 17 00:00:00 2001
From: Utkarsh <utkarsh.maheshwari@altair.com>
Date: Wed, 29 Aug 2018 19:09:01 +0530
Subject: [PATCH] Add PBS Pro as an external resource manager

---
 assembly/pom.xml                                   | 10 ++++
 .../org/apache/spark/deploy/SparkSubmit.scala      | 59 ++++++++++++++++++----
 .../spark/launcher/AbstractCommandBuilder.java     |  1 +
 pom.xml                                            |  7 +++
 4 files changed, 66 insertions(+), 11 deletions(-)

diff --git a/assembly/pom.xml b/assembly/pom.xml
index 68ebfad..86115ad 100644
--- a/assembly/pom.xml
+++ b/assembly/pom.xml
@@ -149,6 +149,16 @@
       </dependencies>
     </profile>
     <profile>
+      <id>pbs</id>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.spark</groupId>
+          <artifactId>spark-pbs_${scala.binary.version}</artifactId>
+          <version>${project.version}</version>
+        </dependency>
+      </dependencies>
+    </profile>
+    <profile>
       <id>kubernetes</id>
       <dependencies>
         <dependency>
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
index d4055cb..d6103ac 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
@@ -230,8 +230,9 @@ private[spark] class SparkSubmit extends Logging {
       case m if m.startsWith("mesos") => MESOS
       case m if m.startsWith("k8s") => KUBERNETES
       case m if m.startsWith("local") => LOCAL
+      case m if m.startsWith("pbs") => PBS
       case _ =>
-        error("Master must either be yarn or start with spark, mesos, k8s, or local")
+        error("Master must either be yarn or start with spark, mesos, k8s, pbs, or local")
         -1
     }
 
@@ -278,6 +279,14 @@ private[spark] class SparkSubmit extends Logging {
       }
     }
 
+    if (clusterManager == PBS) {
+      if (!Utils.classIsLoadable(PBS_CLUSTER_SUBMIT_CLASS) && !Utils.isTesting) {
+        error(
+          "Could not load PBS classes. " +
+            "This copy of Spark may not have been compiled with PBS support.")
+      }
+    }
+
     // Fail fast, the following modes are not supported or applicable
     (clusterManager, deployMode) match {
       case (STANDALONE, CLUSTER) if args.isPython =>
@@ -308,6 +317,8 @@ private[spark] class SparkSubmit extends Logging {
     val isStandAloneCluster = clusterManager == STANDALONE && deployMode == CLUSTER
     val isKubernetesCluster = clusterManager == KUBERNETES && deployMode == CLUSTER
     val isMesosClient = clusterManager == MESOS && deployMode == CLIENT
+    val isPbsClient = clusterManager == PBS && deployMode == CLIENT
+    val isPbsCluster = clusterManager == PBS && deployMode == CLUSTER
 
     if (!isMesosCluster && !isStandAloneCluster) {
       // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
@@ -543,20 +554,20 @@ private[spark] class SparkSubmit extends Logging {
       OptionAssigner(args.archives, YARN, ALL_DEPLOY_MODES, confKey = "spark.yarn.dist.archives"),
 
       // Other options
-      OptionAssigner(args.executorCores, STANDALONE | YARN | KUBERNETES, ALL_DEPLOY_MODES,
+      OptionAssigner(args.executorCores, STANDALONE | YARN | KUBERNETES | PBS, ALL_DEPLOY_MODES,
         confKey = "spark.executor.cores"),
-      OptionAssigner(args.executorMemory, STANDALONE | MESOS | YARN | KUBERNETES, ALL_DEPLOY_MODES,
-        confKey = "spark.executor.memory"),
-      OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,
-        confKey = "spark.cores.max"),
-      OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,
+      OptionAssigner(args.executorMemory, STANDALONE | MESOS | YARN | KUBERNETES | PBS,
+        ALL_DEPLOY_MODES, confKey = "spark.executor.memory"),
+      OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS | KUBERNETES | PBS,
+        ALL_DEPLOY_MODES, confKey = "spark.cores.max"),
+      OptionAssigner(args.files, LOCAL | STANDALONE | MESOS | KUBERNETES | PBS, ALL_DEPLOY_MODES,
         confKey = "spark.files"),
       OptionAssigner(args.jars, LOCAL, CLIENT, confKey = "spark.jars"),
-      OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES, ALL_DEPLOY_MODES,
+      OptionAssigner(args.jars, STANDALONE | MESOS | KUBERNETES | PBS, ALL_DEPLOY_MODES,
         confKey = "spark.jars"),
-      OptionAssigner(args.driverMemory, STANDALONE | MESOS | YARN | KUBERNETES, CLUSTER,
+      OptionAssigner(args.driverMemory, STANDALONE | MESOS | YARN | KUBERNETES | PBS, CLUSTER,
         confKey = "spark.driver.memory"),
-      OptionAssigner(args.driverCores, STANDALONE | MESOS | YARN | KUBERNETES, CLUSTER,
+      OptionAssigner(args.driverCores, STANDALONE | MESOS | YARN | KUBERNETES | PBS, CLUSTER,
         confKey = "spark.driver.cores"),
       OptionAssigner(args.supervise.toString, STANDALONE | MESOS, CLUSTER,
         confKey = "spark.driver.supervise"),
@@ -719,6 +730,29 @@ private[spark] class SparkSubmit extends Logging {
       }
     }
 
+    if (isPbsCluster) {
+      childMainClass = PBS_CLUSTER_SUBMIT_CLASS
+      if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
+        if (args.isPython) {
+          childArgs ++= Array("--primary-py-file", args.primaryResource)
+          childArgs ++= Array("--main-class", "org.apache.spark.deploy.PythonRunner")
+          if (args.pyFiles != null) {
+            childArgs ++= Array("--other-py-files", args.pyFiles)
+          }
+        } else {
+          childArgs ++= Array("--primary-java-resource", args.primaryResource)
+          childArgs ++= Array("--main-class", args.mainClass)
+        }
+      } else {
+        childArgs ++= Array("--main-class", args.mainClass)
+      }
+      if (args.childArgs != null) {
+        args.childArgs.foreach { arg =>
+          childArgs += ("--arg", arg)
+        }
+      }
+    }
+
     // Load any properties specified through --conf and the default properties file
     for ((k, v) <- args.sparkProperties) {
       sparkConf.setIfMissing(k, v)
@@ -882,7 +916,8 @@ object SparkSubmit extends CommandLineUtils with Logging {
   private val MESOS = 4
   private val LOCAL = 8
   private val KUBERNETES = 16
-  private val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL | KUBERNETES
+  private val PBS = 32
+  private val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL | KUBERNETES | PBS
 
   // Deploy modes
   private val CLIENT = 1
@@ -905,6 +940,8 @@ object SparkSubmit extends CommandLineUtils with Logging {
   private[deploy] val STANDALONE_CLUSTER_SUBMIT_CLASS = classOf[ClientApp].getName()
   private[deploy] val KUBERNETES_CLUSTER_SUBMIT_CLASS =
     "org.apache.spark.deploy.k8s.submit.KubernetesClientApplication"
+  private[deploy] val PBS_CLUSTER_SUBMIT_CLASS =
+    "org.apache.spark.deploy.pbs.PbsClusterApplication"
 
   override def main(args: Array[String]): Unit = {
     val submit = new SparkSubmit() {
diff --git a/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java b/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
index ce24400..6e74368 100644
--- a/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
+++ b/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
@@ -159,6 +159,7 @@ abstract class AbstractCommandBuilder {
         "repl",
         "resource-managers/mesos",
         "resource-managers/yarn",
+        "resource-managers/pbs",
         "sql/catalyst",
         "sql/core",
         "sql/hive",
diff --git a/pom.xml b/pom.xml
index 61321a1..882e0d0 100644
--- a/pom.xml
+++ b/pom.xml
@@ -2723,6 +2723,13 @@
     </profile>
 
     <profile>
+      <id>pbs</id>
+      <modules>
+        <module>resource-managers/pbs</module>
+      </modules>
+    </profile>
+
+    <profile>
       <id>hive-thriftserver</id>
       <modules>
         <module>sql/hive-thriftserver</module>
-- 
2.7.4

